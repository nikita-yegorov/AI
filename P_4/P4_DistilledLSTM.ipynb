{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import optim\n",
    "import torchtext\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize)\n",
    "qid = torchtext.data.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2833078384399415\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "target = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n",
    "train = torchtext.data.TabularDataset(path='./data/Quora/train2.csv', format='csv',\n",
    "                                      fields={'question_text': ('text',text),\n",
    "                                              'target': ('target',target)})\n",
    "test = torchtext.data.TabularDataset(path='./data/Quora/test.csv', format='csv',\n",
    "                                     fields={'qid': ('qid', qid),\n",
    "                                             'question_text': ('text', text)})\n",
    "end = time.time()\n",
    "print((end - start)/ 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.build_vocab(train, test, min_freq=3)\n",
    "qid.build_vocab(test)\n",
    "text.vocab.load_vectors(torchtext.vocab.\n",
    "                        Vectors(r'C:\\Users\\Nikita\\PycharmProjects\\Sem_2\\data\\glove.6B.50d.txt'))\n",
    "\n",
    "random.seed(2020)\n",
    "train, val = train.split(split_ratio=0.9, random_state=random.getstate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.data.BucketIterator(dataset=train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sort_key=lambda x: x.text.__len__(),\n",
    "                                               shuffle=True,\n",
    "                                               sort=False)\n",
    "val_iter = torchtext.data.BucketIterator(dataset=val,\n",
    "                                             batch_size=batch_size,\n",
    "                                             sort_key=lambda x: x.text.__len__(),\n",
    "                                             train=False,\n",
    "                                             sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35001\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for train_batch in iter(train_iter):\n",
    "    step += 1\n",
    "for val_batch in iter(val_iter):\n",
    "    step += 1\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, pretrained_lm, padding_idx, static=True, hidden_dim=128, lstm_layer=2, dropout=0.2):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layer,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim * lstm_layer * 2, 1)\n",
    "\n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        y = self.hidden2label(self.dropout(torch.cat([c_n[i, :, :] for i in range(c_n.shape[0])], dim=1)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, pretrained_lm, padding_idx,  hidden_dim=128, lstm_layer=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layer,)\n",
    "        self.hidden2label = nn.Linear(hidden_dim * lstm_layer , 1)\n",
    "\n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        cat = torch.cat([c_n[i, :, :] for i in range(c_n.shape[0])], dim=1)\n",
    "        y = self.hidden2label(cat)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    m = torch.load('model.m')\n",
    "    info = torch.load('model.info')\n",
    "    return m, info\n",
    "def save(m, info):\n",
    "    torch.save(info, 'dm_model1.info')\n",
    "    torch.save(m, 'dm_model1.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_model, bilstm_info = load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(num_epochs, model, teacher_model, loss_func, optimizer, train_iter, val_iter):\n",
    "    \n",
    "    step = 0\n",
    "    train_record = []\n",
    "    val_record = []\n",
    "     \n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = []\n",
    "        for train_batch in iter(train_iter):\n",
    "            step += 1\n",
    "            model.train()\n",
    "            teacher_model.train()\n",
    "            x = train_batch.text.cuda()\n",
    "            y = train_batch.target.type(torch.Tensor).cuda()\n",
    "            model.zero_grad()\n",
    "            pred = model.forward(x)\n",
    "            prob = teacher_model.forward(x).cuda()\n",
    "            loss = loss_function(pred.view(-1), prob.view(-1), y)\n",
    "            tr_loss.append(loss.cpu().data.numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 500 == 0 :\n",
    "                model.eval()\n",
    "                model.zero_grad()\n",
    "                val_loss = []\n",
    "                for val_batch in iter(val_iter):\n",
    "                    \n",
    "                    val_x = val_batch.text.cuda()\n",
    "                    val_y = val_batch.target.type(torch.Tensor).cuda()\n",
    "                    val_pred = model.forward(val_x).view(-1)\n",
    "                    val_prob = teacher_model.forward(val_x).view(-1)\n",
    "                    val_loss.append(loss_function(val_pred,val_prob, val_y).cpu().data.numpy())                    \n",
    "                val_record.append({'step': step, 'loss': np.mean(val_loss)})\n",
    "                train_record.append({'step': step, 'loss':  np.mean(tr_loss)})\n",
    "\n",
    "                print('epoch {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f} '.format(\n",
    "                    epoch, step,  train_record[-1]['loss'], val_record[-1]['loss']))\n",
    "    save(m=model, info={'step': step, 'num_epochs': num_epochs, 'train_loss': train_record[-1]['loss'],\n",
    "        'val_loss': val_record[-1]['loss'], 'tr_record':train_record,  'val_record':val_record})\n",
    "     \n",
    "    return train_record, val_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(output, prob, real_label):\n",
    "    a = 0.5\n",
    "    criterion_mse = torch.nn.MSELoss()\n",
    "    criterion_bce = torch.nn.BCEWithLogitsLoss()\n",
    "    return a*criterion_bce(output, real_label) + (1-a)*criterion_mse(output, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = SimpleLSTM(text.vocab.vectors, lstm_layer=1, padding_idx=text.vocab.stoi[text.pad_token], \n",
    "                        hidden_dim=128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, LSTM_model.parameters()),\n",
    "                    lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 - step 000500 - train_loss 3.0300 - val_loss 2.4937 \n",
      "epoch 00 - step 001000 - train_loss 2.7387 - val_loss 2.7835 \n",
      "epoch 00 - step 001500 - train_loss 2.3866 - val_loss 1.1440 \n",
      "epoch 00 - step 002000 - train_loss 2.0457 - val_loss 1.1578 \n",
      "epoch 00 - step 002500 - train_loss 1.8283 - val_loss 0.9066 \n",
      "epoch 00 - step 003000 - train_loss 1.6791 - val_loss 0.8524 \n",
      "epoch 00 - step 003500 - train_loss 1.5641 - val_loss 0.8488 \n",
      "epoch 00 - step 004000 - train_loss 1.4712 - val_loss 0.7821 \n",
      "epoch 00 - step 004500 - train_loss 1.3934 - val_loss 0.8233 \n",
      "epoch 00 - step 005000 - train_loss 1.3330 - val_loss 0.7374 \n",
      "epoch 00 - step 005500 - train_loss 1.2811 - val_loss 0.6829 \n",
      "epoch 00 - step 006000 - train_loss 1.2316 - val_loss 0.6897 \n",
      "epoch 00 - step 006500 - train_loss 1.1895 - val_loss 0.6422 \n",
      "epoch 00 - step 007000 - train_loss 1.1530 - val_loss 0.6373 \n",
      "epoch 00 - step 007500 - train_loss 1.1191 - val_loss 0.6233 \n",
      "epoch 00 - step 008000 - train_loss 1.0886 - val_loss 0.6858 \n",
      "epoch 00 - step 008500 - train_loss 1.0625 - val_loss 0.5771 \n",
      "epoch 00 - step 009000 - train_loss 1.0355 - val_loss 0.5750 \n",
      "epoch 00 - step 009500 - train_loss 1.0115 - val_loss 0.5971 \n",
      "epoch 00 - step 010000 - train_loss 0.9888 - val_loss 0.5499 \n",
      "epoch 00 - step 010500 - train_loss 0.9676 - val_loss 0.5405 \n",
      "epoch 00 - step 011000 - train_loss 0.9500 - val_loss 0.5452 \n",
      "epoch 00 - step 011500 - train_loss 0.9317 - val_loss 0.5286 \n",
      "epoch 00 - step 012000 - train_loss 0.9144 - val_loss 0.5066 \n",
      "epoch 00 - step 012500 - train_loss 0.8989 - val_loss 0.4945 \n",
      "epoch 00 - step 013000 - train_loss 0.8845 - val_loss 0.5529 \n",
      "epoch 00 - step 013500 - train_loss 0.8706 - val_loss 0.5230 \n",
      "epoch 00 - step 014000 - train_loss 0.8568 - val_loss 0.4826 \n",
      "epoch 00 - step 014500 - train_loss 0.8433 - val_loss 0.4992 \n",
      "epoch 00 - step 015000 - train_loss 0.8311 - val_loss 0.4656 \n",
      "epoch 00 - step 015500 - train_loss 0.8195 - val_loss 0.4691 \n",
      "epoch 00 - step 016000 - train_loss 0.8085 - val_loss 0.4863 \n",
      "epoch 00 - step 016500 - train_loss 0.7986 - val_loss 0.5054 \n",
      "epoch 00 - step 017000 - train_loss 0.7886 - val_loss 0.4664 \n",
      "epoch 00 - step 017500 - train_loss 0.7791 - val_loss 0.4480 \n",
      "epoch 00 - step 018000 - train_loss 0.7695 - val_loss 0.4862 \n",
      "epoch 00 - step 018500 - train_loss 0.7617 - val_loss 0.4493 \n",
      "epoch 00 - step 019000 - train_loss 0.7528 - val_loss 0.4666 \n",
      "epoch 00 - step 019500 - train_loss 0.7452 - val_loss 0.4577 \n",
      "epoch 00 - step 020000 - train_loss 0.7376 - val_loss 0.4429 \n",
      "epoch 00 - step 020500 - train_loss 0.7305 - val_loss 0.4348 \n",
      "epoch 00 - step 021000 - train_loss 0.7238 - val_loss 0.4307 \n",
      "epoch 00 - step 021500 - train_loss 0.7169 - val_loss 0.4348 \n",
      "epoch 00 - step 022000 - train_loss 0.7106 - val_loss 0.4196 \n",
      "epoch 00 - step 022500 - train_loss 0.7046 - val_loss 0.4380 \n",
      "epoch 00 - step 023000 - train_loss 0.6984 - val_loss 0.4213 \n",
      "epoch 00 - step 023500 - train_loss 0.6924 - val_loss 0.4276 \n",
      "epoch 00 - step 024000 - train_loss 0.6870 - val_loss 0.4039 \n",
      "epoch 00 - step 024500 - train_loss 0.6820 - val_loss 0.4173 \n",
      "epoch 00 - step 025000 - train_loss 0.6766 - val_loss 0.4128 \n",
      "epoch 00 - step 025500 - train_loss 0.6718 - val_loss 0.4302 \n",
      "epoch 00 - step 026000 - train_loss 0.6672 - val_loss 0.4124 \n",
      "epoch 00 - step 026500 - train_loss 0.6623 - val_loss 0.3963 \n",
      "epoch 00 - step 027000 - train_loss 0.6578 - val_loss 0.4035 \n",
      "epoch 00 - step 027500 - train_loss 0.6532 - val_loss 0.4312 \n",
      "epoch 00 - step 028000 - train_loss 0.6485 - val_loss 0.4046 \n",
      "epoch 00 - step 028500 - train_loss 0.6444 - val_loss 0.4478 \n",
      "epoch 00 - step 029000 - train_loss 0.6399 - val_loss 0.3874 \n",
      "epoch 00 - step 029500 - train_loss 0.6363 - val_loss 0.3969 \n",
      "epoch 00 - step 030000 - train_loss 0.6323 - val_loss 0.3843 \n",
      "epoch 00 - step 030500 - train_loss 0.6286 - val_loss 0.3790 \n",
      "epoch 00 - step 031000 - train_loss 0.6244 - val_loss 0.3992 \n",
      "epoch 00 - step 031500 - train_loss 0.6210 - val_loss 0.3907 \n",
      "epoch 01 - step 032000 - train_loss 0.3742 - val_loss 0.3890 \n",
      "epoch 01 - step 032500 - train_loss 0.3836 - val_loss 0.3788 \n",
      "epoch 01 - step 033000 - train_loss 0.3799 - val_loss 0.3906 \n",
      "epoch 01 - step 033500 - train_loss 0.3784 - val_loss 0.3745 \n",
      "epoch 01 - step 034000 - train_loss 0.3764 - val_loss 0.3681 \n",
      "epoch 01 - step 034500 - train_loss 0.3753 - val_loss 0.3670 \n",
      "epoch 01 - step 035000 - train_loss 0.3758 - val_loss 0.3797 \n",
      "epoch 01 - step 035500 - train_loss 0.3749 - val_loss 0.3769 \n",
      "epoch 01 - step 036000 - train_loss 0.3752 - val_loss 0.3662 \n",
      "epoch 01 - step 036500 - train_loss 0.3744 - val_loss 0.3643 \n",
      "epoch 01 - step 037000 - train_loss 0.3738 - val_loss 0.3727 \n",
      "epoch 01 - step 037500 - train_loss 0.3736 - val_loss 0.3719 \n",
      "epoch 01 - step 038000 - train_loss 0.3729 - val_loss 0.3732 \n",
      "epoch 01 - step 038500 - train_loss 0.3730 - val_loss 0.3675 \n",
      "epoch 01 - step 039000 - train_loss 0.3716 - val_loss 0.3638 \n",
      "epoch 01 - step 039500 - train_loss 0.3714 - val_loss 0.3635 \n",
      "epoch 01 - step 040000 - train_loss 0.3708 - val_loss 0.3687 \n",
      "epoch 01 - step 040500 - train_loss 0.3698 - val_loss 0.3546 \n",
      "epoch 01 - step 041000 - train_loss 0.3688 - val_loss 0.3580 \n",
      "epoch 01 - step 041500 - train_loss 0.3681 - val_loss 0.3803 \n",
      "epoch 01 - step 042000 - train_loss 0.3677 - val_loss 0.3540 \n",
      "epoch 01 - step 042500 - train_loss 0.3667 - val_loss 0.3530 \n",
      "epoch 01 - step 043000 - train_loss 0.3665 - val_loss 0.3517 \n",
      "epoch 01 - step 043500 - train_loss 0.3661 - val_loss 0.3552 \n",
      "epoch 01 - step 044000 - train_loss 0.3657 - val_loss 0.3471 \n",
      "epoch 01 - step 044500 - train_loss 0.3655 - val_loss 0.3648 \n",
      "epoch 01 - step 045000 - train_loss 0.3652 - val_loss 0.3590 \n",
      "epoch 01 - step 045500 - train_loss 0.3643 - val_loss 0.3568 \n",
      "epoch 01 - step 046000 - train_loss 0.3637 - val_loss 0.4072 \n",
      "epoch 01 - step 046500 - train_loss 0.3630 - val_loss 0.4160 \n",
      "epoch 01 - step 047000 - train_loss 0.3630 - val_loss 0.3657 \n",
      "epoch 01 - step 047500 - train_loss 0.3629 - val_loss 0.3439 \n",
      "epoch 01 - step 048000 - train_loss 0.3622 - val_loss 0.3842 \n",
      "epoch 01 - step 048500 - train_loss 0.3618 - val_loss 0.3474 \n",
      "epoch 01 - step 049000 - train_loss 0.3612 - val_loss 0.3569 \n",
      "epoch 01 - step 049500 - train_loss 0.3609 - val_loss 0.3466 \n",
      "epoch 01 - step 050000 - train_loss 0.3603 - val_loss 0.3579 \n",
      "epoch 01 - step 050500 - train_loss 0.3598 - val_loss 0.3550 \n",
      "epoch 01 - step 051000 - train_loss 0.3594 - val_loss 0.3555 \n",
      "epoch 01 - step 051500 - train_loss 0.3592 - val_loss 0.3453 \n",
      "epoch 01 - step 052000 - train_loss 0.3590 - val_loss 0.3497 \n",
      "epoch 01 - step 052500 - train_loss 0.3588 - val_loss 0.3423 \n",
      "epoch 01 - step 053000 - train_loss 0.3585 - val_loss 0.3557 \n",
      "epoch 01 - step 053500 - train_loss 0.3583 - val_loss 0.3408 \n",
      "epoch 01 - step 054000 - train_loss 0.3578 - val_loss 0.3351 \n",
      "epoch 01 - step 054500 - train_loss 0.3570 - val_loss 0.3443 \n",
      "epoch 01 - step 055000 - train_loss 0.3569 - val_loss 0.3460 \n",
      "epoch 01 - step 055500 - train_loss 0.3567 - val_loss 0.3436 \n",
      "epoch 01 - step 056000 - train_loss 0.3564 - val_loss 0.3459 \n",
      "epoch 01 - step 056500 - train_loss 0.3560 - val_loss 0.3348 \n",
      "epoch 01 - step 057000 - train_loss 0.3555 - val_loss 0.3375 \n",
      "epoch 01 - step 057500 - train_loss 0.3549 - val_loss 0.3359 \n",
      "epoch 01 - step 058000 - train_loss 0.3547 - val_loss 0.3361 \n",
      "epoch 01 - step 058500 - train_loss 0.3541 - val_loss 0.3416 \n",
      "epoch 01 - step 059000 - train_loss 0.3539 - val_loss 0.3694 \n",
      "epoch 01 - step 059500 - train_loss 0.3537 - val_loss 0.3511 \n",
      "epoch 01 - step 060000 - train_loss 0.3536 - val_loss 0.3487 \n",
      "epoch 01 - step 060500 - train_loss 0.3532 - val_loss 0.3380 \n",
      "epoch 01 - step 061000 - train_loss 0.3528 - val_loss 0.3328 \n",
      "epoch 01 - step 061500 - train_loss 0.3526 - val_loss 0.3574 \n",
      "epoch 01 - step 062000 - train_loss 0.3525 - val_loss 0.3335 \n",
      "epoch 01 - step 062500 - train_loss 0.3523 - val_loss 0.3461 \n",
      "epoch 01 - step 063000 - train_loss 0.3518 - val_loss 0.3395 \n",
      "epoch 02 - step 063500 - train_loss 0.3059 - val_loss 0.3247 \n",
      "epoch 02 - step 064000 - train_loss 0.3058 - val_loss 0.3314 \n",
      "epoch 02 - step 064500 - train_loss 0.3078 - val_loss 0.3397 \n",
      "epoch 02 - step 065000 - train_loss 0.3068 - val_loss 0.3272 \n",
      "epoch 02 - step 065500 - train_loss 0.3082 - val_loss 0.3341 \n",
      "epoch 02 - step 066000 - train_loss 0.3100 - val_loss 0.3228 \n",
      "epoch 02 - step 066500 - train_loss 0.3083 - val_loss 0.3335 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 02 - step 067000 - train_loss 0.3081 - val_loss 0.3477 \n",
      "epoch 02 - step 067500 - train_loss 0.3082 - val_loss 0.3393 \n",
      "epoch 02 - step 068000 - train_loss 0.3087 - val_loss 0.3346 \n",
      "epoch 02 - step 068500 - train_loss 0.3090 - val_loss 0.3293 \n",
      "epoch 02 - step 069000 - train_loss 0.3090 - val_loss 0.3293 \n",
      "epoch 02 - step 069500 - train_loss 0.3090 - val_loss 0.3289 \n",
      "epoch 02 - step 070000 - train_loss 0.3091 - val_loss 0.3857 \n",
      "epoch 02 - step 070500 - train_loss 0.3099 - val_loss 0.3333 \n",
      "epoch 02 - step 071000 - train_loss 0.3102 - val_loss 0.3305 \n",
      "epoch 02 - step 071500 - train_loss 0.3105 - val_loss 0.3254 \n",
      "epoch 02 - step 072000 - train_loss 0.3110 - val_loss 0.3300 \n",
      "epoch 02 - step 072500 - train_loss 0.3114 - val_loss 0.3327 \n",
      "epoch 02 - step 073000 - train_loss 0.3117 - val_loss 0.3629 \n",
      "epoch 02 - step 073500 - train_loss 0.3116 - val_loss 0.3601 \n",
      "epoch 02 - step 074000 - train_loss 0.3118 - val_loss 0.3476 \n",
      "epoch 02 - step 074500 - train_loss 0.3117 - val_loss 0.3236 \n",
      "epoch 02 - step 075000 - train_loss 0.3124 - val_loss 0.3351 \n",
      "epoch 02 - step 075500 - train_loss 0.3126 - val_loss 0.3339 \n",
      "epoch 02 - step 076000 - train_loss 0.3124 - val_loss 0.3231 \n",
      "epoch 02 - step 076500 - train_loss 0.3124 - val_loss 0.3334 \n",
      "epoch 02 - step 077000 - train_loss 0.3123 - val_loss 0.3198 \n",
      "epoch 02 - step 077500 - train_loss 0.3119 - val_loss 0.3341 \n",
      "epoch 02 - step 078000 - train_loss 0.3119 - val_loss 0.3310 \n",
      "epoch 02 - step 078500 - train_loss 0.3121 - val_loss 0.3482 \n",
      "epoch 02 - step 079000 - train_loss 0.3116 - val_loss 0.3515 \n",
      "epoch 02 - step 079500 - train_loss 0.3119 - val_loss 0.3354 \n",
      "epoch 02 - step 080000 - train_loss 0.3118 - val_loss 0.3216 \n",
      "epoch 02 - step 080500 - train_loss 0.3117 - val_loss 0.3171 \n",
      "epoch 02 - step 081000 - train_loss 0.3117 - val_loss 0.3158 \n",
      "epoch 02 - step 081500 - train_loss 0.3113 - val_loss 0.3143 \n",
      "epoch 02 - step 082000 - train_loss 0.3112 - val_loss 0.3150 \n",
      "epoch 02 - step 082500 - train_loss 0.3115 - val_loss 0.3332 \n",
      "epoch 02 - step 083000 - train_loss 0.3113 - val_loss 0.3218 \n",
      "epoch 02 - step 083500 - train_loss 0.3113 - val_loss 0.3364 \n",
      "epoch 02 - step 084000 - train_loss 0.3112 - val_loss 0.3410 \n",
      "epoch 02 - step 084500 - train_loss 0.3115 - val_loss 0.3220 \n",
      "epoch 02 - step 085000 - train_loss 0.3116 - val_loss 0.3727 \n",
      "epoch 02 - step 085500 - train_loss 0.3115 - val_loss 0.3315 \n",
      "epoch 02 - step 086000 - train_loss 0.3118 - val_loss 0.3128 \n",
      "epoch 02 - step 086500 - train_loss 0.3116 - val_loss 0.3357 \n",
      "epoch 02 - step 087000 - train_loss 0.3117 - val_loss 0.3196 \n",
      "epoch 02 - step 087500 - train_loss 0.3118 - val_loss 0.3167 \n",
      "epoch 02 - step 088000 - train_loss 0.3118 - val_loss 0.3158 \n",
      "epoch 02 - step 088500 - train_loss 0.3118 - val_loss 0.3157 \n",
      "epoch 02 - step 089000 - train_loss 0.3118 - val_loss 0.3248 \n",
      "epoch 02 - step 089500 - train_loss 0.3118 - val_loss 0.3223 \n",
      "epoch 02 - step 090000 - train_loss 0.3118 - val_loss 0.3191 \n",
      "epoch 02 - step 090500 - train_loss 0.3117 - val_loss 0.3161 \n",
      "epoch 02 - step 091000 - train_loss 0.3114 - val_loss 0.3096 \n",
      "epoch 02 - step 091500 - train_loss 0.3112 - val_loss 0.3179 \n",
      "epoch 02 - step 092000 - train_loss 0.3110 - val_loss 0.3135 \n",
      "epoch 02 - step 092500 - train_loss 0.3110 - val_loss 0.3216 \n",
      "epoch 02 - step 093000 - train_loss 0.3111 - val_loss 0.3214 \n",
      "epoch 02 - step 093500 - train_loss 0.3112 - val_loss 0.3192 \n",
      "epoch 02 - step 094000 - train_loss 0.3110 - val_loss 0.3311 \n",
      "epoch 02 - step 094500 - train_loss 0.3108 - val_loss 0.3198 \n",
      "332.5411785920461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikita\\anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SimpleLSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "t_record, v_record = training(model=LSTM_model, teacher_model=bilstm_model, num_epochs=num_epochs, \n",
    "         loss_func=loss_function, optimizer=optimizer, train_iter=train_iter,\n",
    "        val_iter=val_iter)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/ 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [ t_record[i]['loss']  for i in range(len(t_record))]\n",
    "v = [ v_record[i]['loss']  for i in range(len(t_record))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, len(t))\n",
    "plt.plot(x, t, 'b', label='Training loss')\n",
    "plt.plot(x, v, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.eval()\n",
    "val_pred = []\n",
    "val_true = []\n",
    "val_iter.init_epoch()\n",
    "for val_batch in iter(val_iter):\n",
    "    val_x = val_batch.text.cuda()\n",
    "    val_true += val_batch.target.data.numpy().tolist()\n",
    "    val_pred += torch.sigmoid(LSTM_model.forward(val_x).view(-1)).cpu().data.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.2350 with F1 score: 0.6352\n"
     ]
    }
   ],
   "source": [
    "tmp = [0,0,0] \n",
    "delta = 0\n",
    "for tmp[0] in np.arange(0.01, 0.501, 0.001):\n",
    "    tmp[1] = f1_score(val_true, np.array(val_pred)>tmp[0])\n",
    "    if tmp[1] > tmp[2]:\n",
    "        delta = tmp[0]\n",
    "        tmp[2] = tmp[1]\n",
    "print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
